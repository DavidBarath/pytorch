.. _complex_numbers-doc:

Complex Numbers
===============

Complex Numbers are numbers that can be expressed in the form :math:`a + bi`, where a and b are real numbers,
and *i* is a solution of the equation :math:`x^2 = âˆ’1`. Complex numbers frequently occur in mathematics and
engineering, especially in signal processing. The aim of introducing Complex Tensors is to provide a more natural
user experience for users and libraries (eg. TorchAudio) that currently workaround the lack of Complex Tensors
by using Float Tensors with shape :math:`(..., 2)` where the last dimension contains the real and imaginary values.

We support many functions for Complex Tensors eg. SVD, QR etc. Operations on complex tensors are likely to be
faster than operations on float tensors mimicking them. We also have a `Vec256 class` for `ComplexFloat` and
`ComplexDouble` to benefit from Intel assembly instructions for performing vectorized operations on CPU. In
addition, operations on Complex Tensors utilize specialized kernels for complex provided by LAPACK (for CPU)
and BLAS (for CUDA) or kernels in `torch` specially written for complex tensors. Thus using functions Complex
Tensors will provide performance benefits as opposed to users defining their own functions.

.. warning ::
     Complex Tensors is a beta feature and subject to change.

Creating Complex Tensors
----------------------

We support two complex dtypes: `torch.cfloat` and `torch.cdouble`

::

     >>> x = torch.randn(2,2, dtype=torch.cfloat)
     >>> x
     tensor([[-0.4621-0.0303j, -0.2438-0.5874j],
          [ 0.7706+0.1421j,  1.2110+0.1918j]])

The default dtype for complex tensors is updated based on the current default tensor type.
cdouble if double else float.

<add code>

The following factory functions can be used to create complex tensors:

- :func:`torch.tensor`
- :func:`torch.as_tensor`
- :func:`torch.empty`
- :func:`torch.rand`
- :func:`torch.randn`
- :func:`torch.as_strided`
- :func:`torch.from_numpy`
- :func:`torch.zeros`
- :func:`torch.ones`
- :func:`torch.full`
- :func:`torch.eye`

Transition from the old representation
--------------------------------------

Users who currently worked around the lack of complex tensors with real tensors of shape `(..., 2)`
can easily to switch using the complex tensors in their code using :func:`torch.view_as_complex` and
- :func:`torch.view_as_real`:

::

     >>> x = torch.randn(3, 2)
     >>> x
     tensor([[ 0.6125, -0.1681],
          [-0.3773,  1.3487],
          [-0.0861, -0.7981]])
     >>> y = torch.view_as_complex(x)
     >>> y
     tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])
     >>> torch.view_as_real(y)
     tensor([[ 0.6125, -0.1681],
          [-0.3773,  1.3487],
          [-0.0861, -0.7981]])

Accessing real and imag
-----------------------

<1 line description>

::

     >>> y.real
     tensor([ 0.6125, -0.3773, -0.0861])
     >>> y.imag
     tensor([-0.1681,  1.3487, -0.7981])

Angle and abs
-------------

<1 line description>

::

     >>> x1=torch.tensor([3j, 4+4j])
     >>> x1.abs()
     tensor([3.0000, 5.6569])
     >>> x1.angle()
     tensor([1.5708, 0.7854])

Serialization
-------------

<1 line description>

::

     >>> torch.save(y, 'complex_tensor.pt')
     >>> torch.load('complex_tensor.pt')
     tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])

Autograd
--------

PyTorch supports Autograd for Complex Numbers.

1. :func:`torch.functional.backward` can be used for holomorphic :math:`C -> C` functions.
   For non-holomorphic functions, the gradient is evaluated as if it were holomorphic.
2. :func:`torch.functional.backward` can be used to optimize :math:`C -> R` functions, like
   real-values loss functions of complex parameters :math:`x` by taking steps in the direction
   of conjugate of :math:`x.grad`.
3. mention the current behavior of backward for spectral ops?

For more details, check out the Autograd note. <add the doc link when PR is merged>

> do we need this code or is it better to add it in tutorial?

::

     x = torch.randn(2, 2, 2, dtype=torch.double, requires_grad=True)>>> x = torch.randn(2, 2, 2, dtype=torch.double, requires_grad=True)
     >>> y = x.detach().requires_grad_(True)
     >>> x0 = x.clone()
     >>> x1 = torch.view_as_complex(x0)
     >>> x2 = torch.view_as_real(x1)
     >>> x2.mul_(2)
     tensor([[[ 4.2425, -0.1076],
          [ 3.2731,  2.3156]],

          [[ 4.1179,  0.7358],
          [-1.7711, -0.4389]]], dtype=torch.float64,
          grad_fn=<ViewAsRealBackward>)
     >>> x2.sum().backward()
     >>> y0 = y.clone()
     >>> y0.mul_(2)
     tensor([[[ 4.2425, -0.1076],
          [ 3.2731,  2.3156]],

          [[ 4.1179,  0.7358],
          [-1.7711, -0.4389]]], dtype=torch.float64, grad_fn=<MulBackward0>)
     >>> y0.sum().backward()
     >>> x.grad
     tensor([[[2., 2.],
          [2., 2.]],

          [[2., 2.],
          [2., 2.]]], dtype=torch.float64)
     >>> y.grad
     tensor([[[2., 2.],
          [2., 2.]],

          [[2., 2.],
          [2., 2.]]], dtype=torch.float64)

We also do not support the following subsystems:

Quantization

JIT

Sparse Tensors

distributions, multiprocessing, distributed, ONNX??

If any of these would help your use case, please search if an issue has already been filed (https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22)
and if not, file one(https://github.com/pytorch/pytorch/issues/new/choose).
